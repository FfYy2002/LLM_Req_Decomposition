# -*- coding: utf-8 -*-
"""
è¯„ä¼° LLM æ‹†åˆ†ç»“æœçš„è¦†ç›–è´¨é‡

- è¾“å…¥ï¼šdecomposed_output/decomposed_requirements_*.xlsx
- è¾“å‡ºï¼ševaluation_output/evaluated_decomposed_requirements_*.xlsx
- ç‰¹ç‚¹ï¼šæ¯è¡Œä»…è°ƒç”¨ 1 æ¬¡ LLMï¼Œåˆ¤æ–­æ•´ä½“è¦†ç›–æƒ…å†µ

ä½œè€…ï¼šminefan
æ—¥æœŸï¼š2025å¹´12æœˆ10æ—¥
"""

import os
import pandas as pd
from openai import OpenAI
import threading
import time
import json
import re

# =============================================================================
# é…ç½®
# =============================================================================
INPUT_DIR = "decomposed_output"
OUTPUT_DIR = "evaluation_output"

DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    raise EnvironmentError(
        "âŒ è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY\n"
        "ä¾‹å¦‚ï¼šexport DASHSCOPE_API_KEY='sk-xxx'"
    )

CLIENT = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url="https://dashscope.aliyuncs.com/v1",  # âœ… æ­£ç¡® endpoint
)
MAX_CONCURRENT = 6
semaphore = threading.Semaphore(MAX_CONCURRENT)


# =============================================================================
# å·¥å…·å‡½æ•°ï¼šæå–ç¼–å·éœ€æ±‚ç‚¹
# =============================================================================
def extract_numbered_points(text):
    """ä»æ ‡å‡†åŒ–æ–‡æœ¬ä¸­æå–æ‰€æœ‰ '1. ...' æ ¼å¼çš„éœ€æ±‚ç‚¹"""
    if not isinstance(text, str) or not text.strip():
        return []
    points = re.findall(
        r"^\s*\d+\.\s*(.+?)(?=\n\s*\d+\.|\n\s*è¾¹ç•Œæˆ–å¼‚å¸¸åœºæ™¯å¤„ç†|\n\s*è¯´æ˜ï¼š|$)",
        text,
        re.MULTILINE | re.DOTALL
    )
    cleaned = [p.strip().rstrip("ã€‚.") for p in points if p.strip()]
    return cleaned


# =============================================================================
# å…¨å±€è¯„ä¼° Promptï¼ˆæ ¸å¿ƒï¼‰
# =============================================================================
GLOBAL_EVAL_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„éœ€æ±‚å·¥ç¨‹ä¸“å®¶ï¼Œè¯·ä¸¥æ ¼åˆ¤æ–­ LLM ç”Ÿæˆçš„éœ€æ±‚ç‚¹æ˜¯å¦å®Œæ•´è¦†ç›–äººå·¥æ ‡å‡†ç­”æ¡ˆä¸­çš„æ¯ä¸€æ¡ã€‚

ã€äººå·¥æ ‡å‡†ç­”æ¡ˆã€‘ï¼ˆç¼–å·ä»1å¼€å§‹ï¼‰ï¼š
{gt_list_str}

ã€LLM ç”Ÿæˆç»“æœã€‘ï¼š
{pred_list_str}

ã€åˆ¤æ–­è§„åˆ™ã€‘
1. ä»…å½“ LLM ç‚¹å®Œæ•´è¡¨è¾¾äº†äººå·¥ç‚¹çš„åŠŸèƒ½ã€è§¦å‘æ¡ä»¶ã€ç³»ç»Ÿè¡Œä¸ºï¼ˆå…è®¸è¡¨è¿°ä¸åŒï¼‰ï¼Œæ‰ç®—è¦†ç›–ã€‚
2. å¦‚æœ LLM ç‚¹åªè¦†ç›–éƒ¨åˆ†å†…å®¹ï¼ˆå¦‚åªæâ€œæ ¡éªŒè´¦å·â€ä½†æ²¡æâ€œå¯†ç â€ï¼‰ï¼Œä¸ç®—è¦†ç›–ã€‚
3. ä¸€ä¸ª LLM ç‚¹æœ€å¤šåªèƒ½ç”¨äºè¦†ç›–ä¸€ä¸ªäººå·¥ç‚¹ï¼ˆä¸å¯é‡å¤ä½¿ç”¨ï¼‰ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼š{{"covered_indices": [æ•´æ•°åˆ—è¡¨]}}
- åˆ—è¡¨ä¸­çš„æ•´æ•°æ˜¯è¢«è¦†ç›–çš„äººå·¥ç‚¹ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
- ä¸è¦ä»»ä½•è§£é‡Šã€æ³¨é‡Šã€markdown æˆ–é¢å¤–å†…å®¹

ç¤ºä¾‹è¾“å‡ºï¼š
{{"covered_indices": [1, 3]}}
"""


def evaluate_coverage(gt_points, pred_points):
    """
    è°ƒç”¨ LLM ä¸€æ¬¡ï¼Œåˆ¤æ–­å“ªäº›äººå·¥ç‚¹è¢«è¦†ç›–ã€‚
    è¿”å›ï¼šcovered_indices (list of int, ä»1å¼€å§‹)
    """
    if not gt_points:
        return []

    # é™åˆ¶é•¿åº¦é˜²è¶…é™ï¼ˆqwen-max ä¸Šä¸‹æ–‡è¶³å¤Ÿï¼Œä½†ä¿é™©èµ·è§ï¼‰
    MAX_POINTS = 15
    gt_display = gt_points[:MAX_POINTS]
    pred_display = pred_points[:MAX_POINTS]

    gt_list_str = "\n".join(f"{i+1}. {p}" for i, p in enumerate(gt_display))
    pred_list_str = "\n".join(f"{i+1}. {p}" for i, p in enumerate(pred_display))

    prompt = GLOBAL_EVAL_PROMPT_TEMPLATE.format(
        gt_list_str=gt_list_str,
        pred_list_str=pred_list_str
    )

    for attempt in range(3):
        try:
            with semaphore:
                response = CLIENT.chat.completions.create(
                    model="qwen-max",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.0,
                    timeout=30
                )
            raw_output = response.choices[0].message.content.strip()

            # æå– JSON
            json_match = re.search(r"\{.*\}", raw_output, re.DOTALL)
            if json_build := json_match:
                try:
                    result = json.loads(json_build.group())
                    indices = result.get("covered_indices", [])
                    if isinstance(indices, list):
                        # è¿‡æ»¤æœ‰æ•ˆç¼–å·ï¼ˆ1 ~ len(gt_points)ï¼‰
                        valid_indices = [
                            idx for idx in indices
                            if isinstance(idx, int) and 1 <= idx <= len(gt_points)
                        ]
                        return valid_indices
                except json.JSONDecodeError:
                    pass

            # Fallback: å°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—åˆ—è¡¨
            numbers = re.findall(r"\b\d+\b", raw_output)
            indices = [int(n) for n in numbers if 1 <= int(n) <= len(gt_points)]
            return sorted(set(indices))

        except Exception as e:
            if attempt == 2:
                print(f"    âš ï¸ LLM è°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰: {str(e)[:100]}")
                return []
            time.sleep(2)

    return []


def process_file(filepath: str):
    filename = os.path.basename(filepath)
    print(f"\n[INFO] æ­£åœ¨è¯„ä¼°: {filename}")

    try:
        df = pd.read_excel(filepath)
    except Exception as e:
        print(f"  âœ˜ è¯»å–å¤±è´¥: {e}")
        return

    required_cols = ["AR_ç»†èŠ‚éœ€æ±‚", "LLM_AR_ç»†èŠ‚éœ€æ±‚"]
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        print(f"  âš  è·³è¿‡ï¼šç¼ºå°‘å¿…è¦åˆ— {missing}")
        return

    tps, fns, fps, recalls, precs, f1s = [], [], [], [], [], []
    gt_counts = []
    pred_counts = []

    total_rows = len(df)
    for idx, row in df.iterrows():
        gt_text = row.get("AR_ç»†èŠ‚éœ€æ±‚", "")
        pred_text = row.get("LLM_AR_ç»†èŠ‚éœ€æ±‚", "")

        gt_points = extract_numbered_points(gt_text)
        pred_points = extract_numbered_points(pred_text)

        gt_count = len(gt_points)
        pred_count = len(pred_points)

        covered_indices = evaluate_coverage(gt_points, pred_points)
        tp = len(covered_indices)
        fn = gt_count - tp
        fp = pred_count - tp  # è¿‘ä¼¼ï¼šå‡è®¾æ¯ä¸ªè¦†ç›–æ¶ˆè€—ä¸€ä¸ª pred ç‚¹

        recall = tp / gt_count if gt_count > 0 else 1.0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0

        tps.append(tp)
        fns.append(fn)
        fps.append(fp)
        recalls.append(round(recall, 3))
        precs.append(round(precision, 3))
        f1s.append(round(f1, 3))
        gt_counts.append(gt_count)
        pred_counts.append(pred_count)

        if (idx + 1) % 5 == 0:
            print(f"    å·²å®Œæˆ {idx + 1}/{total_rows} è¡Œ")

    # å†™å…¥æ–°åˆ—
    df["äººå·¥éœ€æ±‚ç‚¹æ•°é‡"] = gt_counts
    df["LLMéœ€æ±‚ç‚¹æ•°é‡"] = pred_counts
    df["TP"] = tps
    df["FN"] = fns
    df["FP"] = fps
    df["æ‹†åˆ†å¬å›ç‡"] = recalls
    df["æ‹†åˆ†ç²¾ç¡®ç‡"] = precs
    df["F1"] = f1s

    # ä¿å­˜
    output_filename = filename.replace("decomposed_requirements_", "evaluated_decomposed_requirements_", 1)
    output_path = os.path.join(OUTPUT_DIR, output_filename)

    try:
        df.to_excel(output_path, index=False, engine="openpyxl")
        print(f"  âœ“ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_path}")
    except Exception as e:
        print(f"  âœ˜ ä¿å­˜å¤±è´¥: {e}")


def main():
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    input_files = [
        f for f in os.listdir(INPUT_DIR)
        if f.endswith(".xlsx")
           and not f.startswith("~$")
           and f.startswith("decomposed_requirements_")
    ]

    if not input_files:
        raise FileNotFoundError(
            f"åœ¨ç›®å½• '{INPUT_DIR}' ä¸­æœªæ‰¾åˆ°ç¬¦åˆ 'decomposed_requirements_*.xlsx' å‘½åè§„èŒƒçš„æ–‡ä»¶"
        )

    print(f"[INFO] å…±å‘ç° {len(input_files)} ä¸ªå¾…è¯„ä¼°æ–‡ä»¶")

    for filename in input_files:
        filepath = os.path.join(INPUT_DIR, filename)
        process_file(filepath)

    # å…¨å±€æ±‡æ€»
    all_dfs = []
    for f in os.listdir(OUTPUT_DIR):
        if f.startswith("evaluated_decomposed_requirements_") and f.endswith(".xlsx"):
            df = pd.read_excel(os.path.join(OUTPUT_DIR, f))
            all_dfs.append(df)

    if all_dfs:
        combined = pd.concat(all_dfs, ignore_index=True)
        total_tp = combined["TP"].sum()
        total_fn = combined["FN"].sum()
        total_fp = combined["FP"].sum()
        macro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        macro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        macro_f1 = 2 * macro_precision * macro_recall / (macro_precision + macro_recall) if (macro_precision + macro_recall) > 0 else 0

        print(f"\nğŸ“Š å…¨å±€è¯„ä¼°ç»“æœ:")
        print(f"   æ€» TP: {total_tp}, FN: {total_fn}, FP: {total_fp}")
        print(f"   å®è§‚å¬å›ç‡: {macro_recall:.3f}")
        print(f"   å®è§‚ç²¾ç¡®ç‡: {macro_precision:.3f}")
        print(f"   å®è§‚ F1: {macro_f1:.3f}")

    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶è¯„ä¼°å·²å®Œæˆï¼ç»“æœä¿å­˜åœ¨ç›®å½•: '{OUTPUT_DIR}/'")


if __name__ == "__main__":
    main()